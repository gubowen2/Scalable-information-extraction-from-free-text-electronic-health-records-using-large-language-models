{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9abe4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import time\n",
    "import re\n",
    "\n",
    "from lm_eval import evaluator, tasks\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig\n",
    "import torch\n",
    "import argparse\n",
    "import os\n",
    "import json\n",
    "\n",
    "\n",
    "from accelerate import init_empty_weights, infer_auto_device_map, dispatch_model, load_checkpoint_in_model\n",
    "from accelerate.utils.modeling import get_balanced_memory\n",
    "from awq.utils.parallel import auto_parallel\n",
    "from awq.quantize.pre_quant import run_awq, apply_awq\n",
    "from awq.quantize.quantizer import pseudo_quantize_model_weight, real_quantize_model_weight\n",
    "from awq.utils.lm_eval_adaptor import LMEvalAdaptor\n",
    "from awq.utils.utils import simple_dispatch_model\n",
    "\n",
    "import string\n",
    "import sys\n",
    "import gc\n",
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f567cfa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520bd109",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_config = {\n",
    "    \"zero_point\": True,  # by default True\n",
    "    \"q_group_size\": 128,  # whether to use group quantization\n",
    "}\n",
    "max_memory = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "704ad631",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model_and_enc(model_path, quantized_file_path, load_quant = True, w_bit = 4):\n",
    "    if not os.path.exists(model_path):  # look into ssd\n",
    "        raise FileNotFoundError(f\"{model_path} not found!\")\n",
    "    print(f\"* Building model {model_path}\")\n",
    "\n",
    "    # all hf model\n",
    "    config = AutoConfig.from_pretrained(model_path, trust_remote_code=True)\n",
    "    print(f\"Config = {config}\")\n",
    "    if \"mpt\" in config.__class__.__name__.lower():\n",
    "        enc = AutoTokenizer.from_pretrained(config.tokenizer_name, trust_remote_code=True)\n",
    "    else:\n",
    "        enc = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "\n",
    "    if load_quant:  # directly load quantized weights\n",
    "        print(\"Loading pre-computed quantized weights...\")\n",
    "        with init_empty_weights():\n",
    "            model = AutoModelForCausalLM.from_config(config=config,\n",
    "                                                     torch_dtype=torch.float16, trust_remote_code=True)\n",
    "        model.config.pretraining_tp = 1\n",
    "        real_quantize_model_weight(\n",
    "            model, w_bit=w_bit, q_config=q_config, init_only=True)\n",
    "        \n",
    "        model.tie_weights()\n",
    "        \n",
    "        # Infer device map\n",
    "        kwargs = {\"max_memory\": max_memory} if len(max_memory) else {}\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            no_split_module_classes=[\n",
    "                \"OPTDecoderLayer\", \"LlamaDecoderLayer\", \"BloomBlock\", \"MPTBlock\", \"DecoderLayer\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        # Load checkpoint in the model\n",
    "        load_checkpoint_in_model(\n",
    "            model,\n",
    "            checkpoint= quantized_file_path,\n",
    "            device_map=device_map,\n",
    "            offload_state_dict=True,\n",
    "        )\n",
    "        # Dispatch model\n",
    "        model = simple_dispatch_model(model, device_map=device_map)\n",
    "\n",
    "        model.eval()\n",
    "    else:  # fp16 to quantized\n",
    "        args.run_awq &= not args.load_awq  # if load_awq, no need to run awq\n",
    "        # Init model on CPU:\n",
    "        kwargs = {\"torch_dtype\": torch.float16, \"low_cpu_mem_usage\": True}\n",
    "        model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_path, config=config, trust_remote_code=True, **kwargs)\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        if args.run_awq:\n",
    "            assert args.dump_awq, \"Please save the awq results with --dump_awq\"\n",
    "                        \n",
    "            awq_results = run_awq(\n",
    "                model, enc,\n",
    "                w_bit=args.w_bit, q_config=q_config,\n",
    "                n_samples=128, seqlen=512,\n",
    "            )\n",
    "            if args.dump_awq:\n",
    "                dirpath = os.path.dirname(args.dump_awq)\n",
    "                os.makedirs(dirpath, exist_ok=True)\n",
    "                \n",
    "                torch.save(awq_results, args.dump_awq)\n",
    "                print(\"AWQ results saved at\", args.dump_awq)\n",
    "                \n",
    "            exit(0)\n",
    "                \n",
    "        if args.load_awq:\n",
    "            print(\"Loading pre-computed AWQ results from\", args.load_awq)\n",
    "            awq_results = torch.load(args.load_awq, map_location=\"cpu\")\n",
    "            apply_awq(model, awq_results)\n",
    "\n",
    "        # weight quantization\n",
    "        if args.w_bit is not None:\n",
    "            if args.q_backend == \"fake\":\n",
    "                assert args.dump_quant is None, \\\n",
    "                    \"Need to use real quantization to dump quantized weights\"\n",
    "                pseudo_quantize_model_weight(\n",
    "                    model, w_bit=args.w_bit, q_config=q_config\n",
    "                )\n",
    "            elif args.q_backend == \"real\":  # real quantization\n",
    "                real_quantize_model_weight(\n",
    "                    model, w_bit=args.w_bit, q_config=q_config\n",
    "                )\n",
    "                if args.dump_quant:\n",
    "                    dirpath = os.path.dirname(args.dump_quant)\n",
    "                    os.makedirs(dirpath, exist_ok=True)\n",
    "                    \n",
    "                    print(\n",
    "                        f\"Saving the quantized model at {args.dump_quant}...\")\n",
    "                    torch.save(model.cpu().state_dict(), args.dump_quant)\n",
    "                    exit(0)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            \n",
    "        # Move the model to GPU (as much as possible) for LM evaluation\n",
    "        kwargs = {\"max_memory\": get_balanced_memory(model, max_memory if len(max_memory) > 0 else None)}\n",
    "        device_map = infer_auto_device_map(\n",
    "            model,\n",
    "            # TODO: can we remove this?\n",
    "            no_split_module_classes=[\n",
    "                \"OPTDecoderLayer\", \"LlamaDecoderLayer\", \"BloomBlock\", \"MPTBlock\", \"DecoderLayer\"],\n",
    "            **kwargs\n",
    "        )\n",
    "        model = dispatch_model(model, device_map=device_map)\n",
    "\n",
    "    return model, enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47026630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_question_list(question_text_file_path):\n",
    "    # Open the file in read mode\n",
    "    with open(question_text_file_path, 'r') as file:\n",
    "        # Read the content of the file\n",
    "        text = file.read()\n",
    "    question_list = text.split(\"\\n\\n\")\n",
    "    question_list = [input_string.split('. ', 1)[-1] for input_string in question_list]\n",
    "    return question_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50bc57e",
   "metadata": {},
   "outputs": [],
   "source": [
    "question_header_list = [\"the patientâ€™s marital status information is:\",\n",
    "                \"the patient's children information is:\",\n",
    "                \"the patient's smoking or tobacco usage information is:\",\n",
    "                \"the patient's drinking or alcohol usage information is:\",\n",
    "                \"the patient's drug usage information is:\",\n",
    "                \"the patient's living information is:\",\n",
    "                \"the patient's employment information is:\",\n",
    "                \"the patient's education information is:\",\n",
    "                \"the patient's exercising information is:\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857a1b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_default_prompt(text, question):\n",
    "    header = '''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [PROMPT]'''\n",
    "    prompt = '''Given the following text about a patient's social documentation.\n",
    "                    [TEXT]\n",
    "                    Please answer the question: [QUESTION]\n",
    "                    ASSISTANT: '''\n",
    "    result = header.replace(\"[PROMPT]\", prompt)\n",
    "    result = result.replace(\"[TEXT]\", text)\n",
    "    result = result.replace(\"[QUESTION]\", question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d0d9a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Q6_prompt(text, question):\n",
    "    header = '''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [PROMPT]'''\n",
    "    prompt = '''Given the following text about a patient's social documentation.\n",
    "                    [TEXT]\n",
    "                    Please answer the question: [QUESTION]\n",
    "                    Please DO NOT infer the answer according to the text. ONLY give your answers based on the ACTUAL text.\n",
    "                    Your answer should be \"Yes\" IF AND ONLY IF the text EXPLICITLY mentions that the patient lives alone.\n",
    "                    Your answer should be \"No\" IF AND ONLY IF the text EXPLICITLY mentions that the patient lives with someone.\n",
    "                    Otherwise, your answer MUST be \"Not mentioned\".\n",
    "                    ASSISTANT: '''\n",
    "    result = header.replace(\"[PROMPT]\", prompt)\n",
    "    result = result.replace(\"[TEXT]\", text)\n",
    "    result = result.replace(\"[QUESTION]\", question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76629d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Q7_prompt(text, question):\n",
    "    header = '''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [PROMPT]'''\n",
    "    prompt = '''Given the following text about a patient's social documentation.\n",
    "                    [TEXT]\n",
    "                    Please answer the question: [QUESTION]\n",
    "                    Your answer should be \"Employed\" if the text mentions that the patient has a full-time or part-time job (even if the patient retired or was jobless before).\n",
    "                    Your answer should be \"Jobless\" if the text mentions that the patient is not working, is a homemaker, or is on disability.\n",
    "                    Your answer should be \"Retired\" if the text mentions that the patient is retired, is a former employee or used to be an employee.\n",
    "                    ASSISTANT: '''\n",
    "    result = header.replace(\"[PROMPT]\", prompt)\n",
    "    result = result.replace(\"[TEXT]\", text)\n",
    "    result = result.replace(\"[QUESTION]\", question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a983df86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_second_Q7_prompt(text):\n",
    "    header = '''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [PROMPT]'''\n",
    "    prompt = '''Given the following text about a patient's social documentation.\n",
    "                    [TEXT]\n",
    "                    Please answer the question: Does the text mention the patient's occupation?\n",
    "                    Your answer must be one of the following:\n",
    "                    - Yes\n",
    "                    - No\n",
    "                    ASSISTANT: '''\n",
    "    result = header.replace(\"[PROMPT]\", prompt)\n",
    "    result = result.replace(\"[TEXT]\", text)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a851211",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_few_shot_learning_prompt(text, question, few_shot_learning_prompt_path):\n",
    "    header = '''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [PROMPT]'''\n",
    "    with open(few_shot_learning_prompt_path, 'r') as file:\n",
    "        # Read the content of the file\n",
    "        prompt = file.read()\n",
    "    result = header.replace(\"[PROMPT]\", prompt)\n",
    "    result = result.replace(\"[TEXT]\", text)\n",
    "    result = result.replace(\"[QUESTION]\", question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc3eacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_default_secondary_prompt(text, question, question_id):\n",
    "    header = '''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [PROMPT]'''\n",
    "    prompt = '''Suppose that [HEADER] [TEXT].\n",
    "                [QUESTION]\n",
    "                ASSISTANT: '''\n",
    "    result = header.replace(\"[PROMPT]\", prompt)\n",
    "    result = result.replace(\"[HEADER]\", question_header_list[question_id])\n",
    "    result = result.replace(\"[TEXT]\", text)\n",
    "    result = result.replace(\"[QUESTION]\", question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9735ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_Q7_secondary_prompt(text, question):\n",
    "    header = '''A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions. USER: [PROMPT]'''\n",
    "    prompt = '''Given the following text:\n",
    "                [TEXT]\n",
    "                [QUESTION]\n",
    "                ASSISTANT: '''\n",
    "    result = header.replace(\"[PROMPT]\", prompt)\n",
    "    result = result.replace(\"[TEXT]\", text)\n",
    "    result = result.replace(\"[QUESTION]\", question)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a38365",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(model_name):\n",
    "    supported_models = [\"openchat_3.5\", \n",
    "                          \"zephyr-7b-beta\", \n",
    "                          \"vicuna-7b-v1.5\", \n",
    "                          \"Llama-2-7b-chat-hf\", \n",
    "                          \"vicuna-13b-v1.5\", \n",
    "                          \"WizardLM-13B-V1.2\", \n",
    "                          \"Llama-2-13b-chat-hf\", \n",
    "                        \"vicuna-33b-v1.3\", \n",
    "                        \"WizardLM-70B-V1.0\", \n",
    "                        \"Llama-2-70b-chat-hf\"]\n",
    "    if (model_name.split(\"-w\")[0] in supported_models):\n",
    "        if (\"-w\" in model_name): # This is a quantized model\n",
    "            model_path = f\"../.cache/huggingface/transformers/{model_name.split('-w')[0]}\"\n",
    "            quantized_file_path = f\"quant_cache/{model_name}.pt\"\n",
    "            model, tokenizer = build_model_and_enc(model_path, quantized_file_path = quantized_file_path)\n",
    "        else:\n",
    "            tokenizer = AutoTokenizer.from_pretrained(f\"../.cache/huggingface/transformers/{model_name}\")\n",
    "            model = AutoModelForCausalLM.from_pretrained(f\"../.cache/huggingface/transformers/{model_name}\")\n",
    "    else:\n",
    "        raise NotImplementedError(f\"[ERROR]: Model {model_name} not supported\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a862a437",
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline(text, question_id):\n",
    "\n",
    "    def Q1_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines the patient's marital status based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(widowed)\\b', text, re.IGNORECASE):\n",
    "            return 'Widowed'\n",
    "        elif re.search(r'\\b(divorced|separated)\\b', text, re.IGNORECASE):\n",
    "            return 'Divorced'\n",
    "        elif re.search(r'\\b(single|no spouse|partner|boyfriend|girlfriend|long-term partner)\\b', text, re.IGNORECASE):\n",
    "            return 'Single'\n",
    "        elif re.search(r'\\b(married)\\b', text, re.IGNORECASE):\n",
    "            return 'Married'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q2_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines the number of children the patient has based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(no children|0 children|zero children|no child|0 child|zero child)\\b', text, re.IGNORECASE):\n",
    "            return '0'\n",
    "        elif re.search(r'\\b(1 child|1 son|1 daughter|one child|one son|one daughter)\\b', text, re.IGNORECASE):\n",
    "            return '1'\n",
    "        elif re.search(r'\\b(2 children|2 sons|2 daughters|two children|two sons|two daughters)\\b', text, re.IGNORECASE):\n",
    "            return '2'\n",
    "        elif re.search(r'\\b(3 children|3 sons|3 daughters|three children|three sons|three daughters)\\b', text, re.IGNORECASE):\n",
    "            return '3'\n",
    "        elif re.search(r'\\b(4 children|4 sons|4 daughters|four children|four sons|four daughters)\\b', text, re.IGNORECASE):\n",
    "            return '4'\n",
    "        elif re.search(r'\\b(5 children|6 children|7 children|8 children|9 children|10 children|five children|six children|seven children|eight children|nine children|ten children)\\b', text, re.IGNORECASE):\n",
    "            return '5 or more'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q3_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines if the patient currently uses tobacco based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(never used tobacco|quit smoking|quit tobacco|quit smoke|past tobacco use|never smoke|no smoke)\\b', text, re.IGNORECASE):\n",
    "            return 'No'\n",
    "        elif re.search(r'\\b(currently smokes|cigarettes|cigars|smokeless tobacco|tobacco use|smoke)\\b', text, re.IGNORECASE):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q4_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines if the patient currently consumes alcohol based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(never consumed alcohol|sober|no ETOH)\\b', text, re.IGNORECASE):\n",
    "            return 'No'\n",
    "        elif re.search(r'\\b(consumes alcohol|drinks alcohol|ETOH)\\b', text, re.IGNORECASE):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q5_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines if the patient currently uses illicit drugs based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(never used drugs|sober from drugs|past drug use|deny illicit drugs)\\b', text, re.IGNORECASE):\n",
    "            return 'No'\n",
    "        elif re.search(r'\\b(illicit drugs|uses drugs|cocaine|marijuana|substance abuse)\\b', text, re.IGNORECASE):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q6_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines if the patient lives alone based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(alone)\\b', text, re.IGNORECASE):\n",
    "            return 'Yes'\n",
    "        elif re.search(r'\\b(with husband|with wife|with child|with children|with son|with daughter|with boyfriend|with girlfriend|with friend|with grandparents|with grandmother|with grandfather|with uncle|with aunt|with parents|with father|with mother|with dad|with mom)\\b', text, re.IGNORECASE):\n",
    "            return 'No'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q7_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines the patient's employment status based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(full-time|part-time|employed|work as|employ)\\b', text, re.IGNORECASE):\n",
    "            return 'Employed'\n",
    "        elif re.search(r'\\b(stay-at-home|at home|unemployed|on disability|homemaker|not work|no work|jobless)\\b', text, re.IGNORECASE):\n",
    "            return 'Jobless'\n",
    "        elif re.search(r'\\b(retire|former employee|worked|used to work)\\b', text, re.IGNORECASE):\n",
    "            return 'Retired'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q8_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines the patient's highest education level based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(elementary school|1st grade|2nd grade|3rd grade|4th grade|5th grade)\\b', text, re.IGNORECASE):\n",
    "            return 'Elementary school'\n",
    "        elif re.search(r'\\b(middle school|6th grade|7th grade|8th grade)\\b', text, re.IGNORECASE):\n",
    "            return 'Middle school'\n",
    "        elif re.search(r'\\b(high school|9th grade|10th grade|11th grade)\\b', text, re.IGNORECASE):\n",
    "            return 'High school'\n",
    "        elif re.search(r'\\b(college|Associates|Bachelors|BS|BA|freshman|sophomore|junior|senior|post-bacc)\\b', text, re.IGNORECASE):\n",
    "            return 'College'\n",
    "        elif re.search(r'\\b(graduate school|grad school|Masters degree|MS|MBA|MPH|MENG|Doctoral degree|PHD|MD)\\b', text, re.IGNORECASE):\n",
    "            return 'Graduate school'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    def Q9_pattern_matching(text):\n",
    "        \"\"\"\n",
    "        Determines if the patient exercises based on the given text.\n",
    "        \"\"\"\n",
    "        if re.search(r'\\b(does not exercise|never exercise|not exercising)\\b', text, re.IGNORECASE):\n",
    "            return 'No'\n",
    "        elif re.search(r'\\b(used to exercise|used to walk)\\b', text, re.IGNORECASE):\n",
    "            return 'In the past'\n",
    "        elif re.search(r'\\b(exercise|walk|run|dance|gym|treadmill|yoga)\\b', text, re.IGNORECASE):\n",
    "            return 'Yes'\n",
    "        else:\n",
    "            return 'Not mentioned'\n",
    "\n",
    "    if (question_id == 1):\n",
    "        return Q1_pattern_matching(text)\n",
    "    elif (question_id == 2):\n",
    "        return Q2_pattern_matching(text)\n",
    "    elif (question_id == 3):\n",
    "        return Q3_pattern_matching(text)\n",
    "    elif (question_id == 4):\n",
    "        return Q4_pattern_matching(text)\n",
    "    elif (question_id == 5):\n",
    "        return Q5_pattern_matching(text)\n",
    "    elif (question_id == 6):\n",
    "        return Q6_pattern_matching(text)\n",
    "    elif (question_id == 7):\n",
    "        return Q7_pattern_matching(text)\n",
    "    elif (question_id == 8):\n",
    "        return Q8_pattern_matching(text)\n",
    "    elif (question_id == 9):\n",
    "        return Q9_pattern_matching(text)\n",
    "    else:\n",
    "        raise ValueError(f\"[ERROR]: question_id {question_id} is invalid. question_id must range from 1 to 9.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ff6075",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_text(model, tokenizer, prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=2048)\n",
    "    question_and_response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "    response = question_and_response.split(\"ASSISTANT: \")[-1]\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc261b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_choices(question):\n",
    "    choices = question.split(\"\\n\")[2:]\n",
    "    choices = [choice.replace(\"- \", \"\") for choice in choices]\n",
    "    return choices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a4727a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(s):\n",
    "    return ''.join(c if c not in string.punctuation else ' ' for c in s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bafd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_response(response, choices_list):\n",
    "    match = []\n",
    "    for choice in choices_list:\n",
    "        if (len(choice.split(\" \")) == 1): # Choice is a single word\n",
    "            if choice.lower() in remove_punctuation(response).lower().replace(\"\\n\", \" \").split(\" \"):\n",
    "                match.append(choice)\n",
    "        else: # Choice has multiple words\n",
    "            if choice.lower() in remove_punctuation(response).lower().replace(\"\\n\", \" \"):\n",
    "                match.append(choice)\n",
    "    return match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be0a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_and_check(response, choices_list, question_list, question_id, info_header, level, bumpy_question_threshold):\n",
    "    print(f\"[INFO] (level {level}) {info_header}: Response before refinement: {response}\")\n",
    "    # Check if response is empty, contains only whitespaces, or any of the specified phrases or contains no numbers or English characters\n",
    "    no_numbers_or_english_pattern = re.compile('^[^0-9A-Za-z]*$')\n",
    "    if not response.strip() or re.match(no_numbers_or_english_pattern, response) or any(substring in response.lower() for substring in [\"not mentioned\", \"unknown\", \"not available\", \"unavailable\", \"not provided\", \"none\", \"not include\", \"does not provide any information\", \"cannot determine\"]):\n",
    "        print(f\"[INFO] (level {level}) {info_header}: Refinement SUCCESS! Response before refinement: Not mentioned\")\n",
    "        return \"Not mentioned\"\n",
    "\n",
    "    # Regular expression patterns to match Chinese, Japanese, and Korean characters\n",
    "    chinese_pattern = re.compile('[\\u4e00-\\u9fff]+')\n",
    "    japanese_pattern = re.compile('[\\u3040-\\u309F\\u30A0-\\u30FF\\u4E00-\\u9FFF]+')\n",
    "    korean_pattern = re.compile('[\\uAC00-\\uD7AF]+')\n",
    "\n",
    "    # Search for Chinese, Japanese, or Korean characters in the string\n",
    "    if re.search(chinese_pattern, response) or re.search(japanese_pattern, response) or re.search(korean_pattern, response):\n",
    "        print(f\"[INFO] (level {level}) {info_header}: Refinement SUCCESS! Response before refinement: Not mentioned\")\n",
    "        return \"Not mentioned\"\n",
    "    \n",
    "    if (len(response.split(\"\\n\")) > 1): # At least for openchat-3.5, most of its answers are on the first line\n",
    "        match = match_response(response.split(\"\\n\")[0], choices_list)\n",
    "        if (len(match) != 1):\n",
    "            match = match_response(response, choices_list)\n",
    "    else:\n",
    "        match = match_response(response, choices_list)\n",
    "        \n",
    "    if (len(match) > 1):\n",
    "        print(f\"[INFO] (level {level}) {info_header}: Mutiple matches found! match = {match}\")\n",
    "    elif (len(match) == 1):\n",
    "        print(f\"[INFO] (level {level}) {info_header}: Single match found! match = {match}\")\n",
    "    else:\n",
    "        print(f\"[INFO] (level {level}) {info_header}: No match found!\")\n",
    "    \n",
    "    if len(match) == 1:\n",
    "        print(f\"[INFO] (level {level}) {info_header}: Refinement SUCCESS! Response after refinement: {match[0]}\")\n",
    "        return match[0]\n",
    "    else:\n",
    "        if (level == 0):\n",
    "            print(f\"[INFO] (level {level}) {info_header}: Cannot resolve this invalid case using regular expression refinement. Retry with secondary prompt.\")\n",
    "            if (question_id + 1 == 7 and len(question_list) == 1):\n",
    "                secondary_prompt = build_Q7_secondary_prompt(response, question_list[0])\n",
    "            else:\n",
    "                secondary_prompt = build_default_secondary_prompt(response, question_list[question_id], question_id)\n",
    "            print(f\"[INFO] (level {level}) {info_header}: The secondary prompt is\\n{secondary_prompt}\")\n",
    "            question_start_time = time.time()\n",
    "            response = LLM_text(model, tokenizer, secondary_prompt)\n",
    "            question_end_time = time.time()\n",
    "            question_time = question_end_time - question_start_time\n",
    "            print(f\"[INFO] (level {level}) {info_header}: Processing this question took {question_time} seconds\")\n",
    "            if (bumpy_question_threshold > 500):\n",
    "                print(f\"[INFO] (level {level}) {info_header}: This is a bumpy question, which takes {question_time} seconds to finish (Bumpy question threshold = {bumpy_question_threshold})\")\n",
    "            response = refine_and_check(response, choices_list, question_list, question_id, info_header, level + 1, bumpy_question_threshold)\n",
    "            if (\"Invalid response:\" in response):\n",
    "                print(f\"[INFO] (level {level}) {info_header}: Refinement FAIL! Response after refinement: {response}\")\n",
    "            else:\n",
    "                print(f\"[INFO] (level {level}) {info_header}: Refinement SUCCESS! Response after refinement: {response}\")\n",
    "            return response\n",
    "        else:\n",
    "            response = \"Invalid response: \" + response\n",
    "            print(f\"[INFO] (level {level}) {info_header}: Refinement FAIL! Response after refinement: {response}\")\n",
    "            return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efeeb0fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LLM_pipeline(model_name, question_text_file_path, COT_question_text_file_path, SDoH_file_path, first_n, specific_patients, few_shot_learning_question_list, few_shot_learning_prompt_path_list, specific_COT_questions_list, specific_questions_list, bumpy_question_threshold, test_set):\n",
    "    if (test_set):\n",
    "        sys.stdout = open(f'log ({model_name}) (Test).txt', 'a')\n",
    "    else:\n",
    "        sys.stdout = open(f'log ({model_name}).txt', 'a')\n",
    "    print(\"****************************** LOG START ******************************\")\n",
    "    print(f\"[INFO]: Pipeline input parameters:\\nmodel_name = {model_name},\\nquestion_text_file_path = {question_text_file_path},\\nCOT_question_text_file_path = {COT_question_text_file_path},\\nSDoH_file_path = {SDoH_file_path},\\nfirst_n = {first_n},\\nspecific_patients = {specific_patients},\\nfew_shot_learning_question_list = {few_shot_learning_question_list},\\nfew_shot_learning_prompt_path_list = {few_shot_learning_prompt_path_list},\\nspecific_COT_questions_list = {specific_COT_questions_list},\\nspecific_questions_list = {specific_questions_list},\\nbumpy_question_threshold = {bumpy_question_threshold},\\ntest_set = {test_set}\")\n",
    "    if (few_shot_learning_question_list is None and few_shot_learning_prompt_path_list is not None) or \\\n",
    "       (few_shot_learning_question_list is not None and few_shot_learning_prompt_path_list is None):\n",
    "        raise ValueError(\"[ERROR]: Few_shot_learning_question_list and few_shot_learning_prompt_path_list must be both None or both not None\")\n",
    "        \n",
    "    if (few_shot_learning_question_list is not None and few_shot_learning_prompt_path_list is not None):\n",
    "        if (type(few_shot_learning_question_list) != list) or (type(few_shot_learning_prompt_path_list) != list):\n",
    "            raise ValueError(\"[ERROR]: Few_shot_learning_questions and few_shot_learning_prompt_path_list must be lists\")\n",
    "\n",
    "        if len(few_shot_learning_question_list) != len(few_shot_learning_prompt_path_list):\n",
    "            raise ValueError(\"[ERROR]: Few_shot_learning_questions and few_shot_learning_prompt_path_list must have the same length\")\n",
    "        \n",
    "    if specific_questions_list is not None and type(specific_questions_list) != list:\n",
    "        raise ValueError(\"[ERROR]: Specific_questions_list must be a list\")\n",
    "\n",
    "    if (first_n is None and specific_patients is None):\n",
    "        raise ValueError(\"[WARNING]: The current setup will run ALL patients in the data set, which will take a significant amount of time for this program to finish. To reduce the number of patients, please set 'first_n' or 'specific_patients'\")\n",
    "    \n",
    "    if (model_name == \"baseline\" and specific_COT_questions_list is not None):\n",
    "        specific_COT_questions_list = None\n",
    "        print(\"[WARNING]: The baseline does not support COT questions. Only LLMs have COT questions. Changing specific_COT_questions_list to None.\")\n",
    "        \n",
    "    if (model_name == \"baseline\" and few_shot_learning_question_list is not None):\n",
    "        few_shot_learning_question_list = None\n",
    "        print(\"[WARNING]: The baseline does not support few-shot learning questions. Only LLMs have few-shot learning questions. Changing few_shot_learning_question_list to None.\")\n",
    "        \n",
    "    print(f\"[INFO]: Loading model {model_name}\")\n",
    "    #model, tokenizer = get_model(model_name)\n",
    "    # The \"result_df\" is for autograder, the \"time_df\" is for computation resource calculation\n",
    "    result_df = pd.DataFrame()\n",
    "    time_df = pd.DataFrame()\n",
    "    data = pd.read_csv(SDoH_file_path)\n",
    "    if (specific_patients is not None and type(specific_patients) == list):\n",
    "        data = data[data[\"PATIENT_NUM\"].isin(specific_patients)]\n",
    "        print(f\"[INFO]: Found {len(data)} out of {len(specific_patients)} patients\")\n",
    "    if first_n is not None:\n",
    "        if (first_n <= len(data)):\n",
    "            data = data.head(first_n)\n",
    "            print(f\"[INFO] Processing the first {first_n} patients\")\n",
    "        else:\n",
    "            print(f\"[WARNING] Try to process the first {first_n} patients, but there are only {len(data)} patients in the current data set. Will process all {len(data)} patients instead.\")\n",
    "\n",
    "    MRN_list = data[\"PATIENT_NUM\"].tolist()\n",
    "    SDoH_text_list = data[\"OBSERVATION_BLOB\"].tolist()\n",
    "    update_date_list = data[\"UPDATE_DATE\"].tolist()\n",
    "    num_patients = len(data)\n",
    "    question_list = generate_question_list(question_text_file_path)\n",
    "    COT_question_list = generate_question_list(COT_question_text_file_path)\n",
    "    if specific_questions_list is not None:\n",
    "        num_questions = len(specific_questions_list)\n",
    "    else:\n",
    "        num_questions = len(question_list)\n",
    "    question_display_list_detailed = [f\"Q{i + 1}. {question}\" for i, question in enumerate(question_list)] # If you want to show the choices, use this line of code\n",
    "    simplified_question_list = [question.split('\\n')[0] for question in question_list]\n",
    "    COT_simplified_question_list = [question.split('\\n')[0] for question in COT_question_list]\n",
    "    question_display_list = [f\"Q{i + 1}. {question}\" for i, question in enumerate(simplified_question_list)] # If you only want the question, use this line of code\n",
    "    COT_question_display_list = [f\"Q{i + 1}. {question}\" for i, question in enumerate(COT_simplified_question_list)]\n",
    "    result_df[model_name] = [\"Text\"] + ([question_display_list_detailed[i - 1] for i in specific_questions_list] if specific_questions_list is not None else question_display_list_detailed)\n",
    "    time_df[model_name] = ([question_display_list_detailed[i - 1] for i in specific_questions_list] if specific_questions_list is not None else question_display_list_detailed)\n",
    "    for i, (MRN, text) in enumerate(zip(MRN_list, SDoH_text_list)):\n",
    "        patient_start_time = time.time()\n",
    "        print(f\"[INFO]: Processing patient {i + 1} out of {num_patients} (MRN: {MRN})\")\n",
    "        responses = []\n",
    "        time_usage = []\n",
    "        counter = 0\n",
    "        for j, (question, COT_question) in enumerate(zip(question_list, COT_question_list)):\n",
    "            if (model_name != \"baseline\" and j in [1, 6]):\n",
    "                refined_text = text.split(\"  Socioeconomic\")[0] # The part after \"  Socioeconomic  Occupational\" will confuse the LLM on questions 2 and 7\n",
    "                if (j == 6 and \"  Socioeconomic\" in text):\n",
    "                    refined_text += text.split(\"  Socioeconomic\", 1)[-1].replace(\"Occupational\", \"\").replace(\"Occupation\", \"\")\n",
    "            else:\n",
    "                refined_text = text\n",
    "            if specific_questions_list is not None and (j + 1) not in specific_questions_list:\n",
    "                continue\n",
    "            question_start_time = time.time()\n",
    "            print(f\"[INFO]: Processing question {counter + 1} out of {num_questions} (Prompt: {'Few shot learning' if few_shot_learning_question_list is not None and (j + 1) in few_shot_learning_question_list else 'Default'}) (Question: {simplified_question_list[j]})\")\n",
    "            info_header = f\"(Patient {i + 1} (MRN: {MRN}) Question {counter + 1} (Question: {question_display_list[j]}) (Prompt: {'Few shot learning' if few_shot_learning_question_list is not None and (j + 1) in few_shot_learning_question_list else 'Default'}))\"\n",
    "            COT_info_header = f\"(Patient {i + 1} (MRN: {MRN}) Question {counter + 1} (Question: {COT_question_display_list[j]}) (Prompt: Default))\"\n",
    "            if (specific_COT_questions_list is not None and (j + 1) in specific_COT_questions_list):\n",
    "                print(f\"[INFO]: Question {counter + 1} is a COT question. Running the COT question.\")\n",
    "                COT_start_time = time.time()\n",
    "                COT_prompt = build_default_prompt(refined_text, COT_question)\n",
    "                print(f\"[INFO] {COT_info_header}: The COT prompt is\\n{COT_prompt}\")\n",
    "                COT_response = LLM_text(model, tokenizer, COT_prompt)\n",
    "                COT_choices_list = generate_choices(COT_question)\n",
    "                COT_refined_response = refine_and_check(COT_response, COT_choices_list, COT_question_list, j, COT_info_header, 0, bumpy_question_threshold)\n",
    "                print(f\"[INFO] {COT_info_header}: The refined model response for the COT question is {COT_refined_response}\")\n",
    "                COT_end_time = time.time()\n",
    "                COT_time = COT_end_time - COT_start_time\n",
    "                print(f\"[INFO] {COT_info_header}: Running the COT question took {COT_time} seconds\")\n",
    "                if (COT_time > bumpy_question_threshold):\n",
    "                    print(f\"[INFO] {COT_info_header}: This is a bumpy question, which takes {COT_time} seconds to finish (Bumpy question threshold = {bumpy_question_threshold})\")\n",
    "                if (COT_refined_response == \"No\"):\n",
    "                    print(f\"[INFO] {COT_info_header}: Answer determined from the COT question to be 'Not mentioned'. No need to run the actual question\")\n",
    "                    refined_response = \"Not mentioned\"\n",
    "                else:\n",
    "                    print(f\"[INFO] {COT_info_header}: Answer cannot be determined from the COT question. Running the actual question\")\n",
    "                    if (few_shot_learning_question_list is not None and (j + 1) in few_shot_learning_question_list):\n",
    "                        prompt = build_few_shot_learning_prompt(refined_text, question, few_shot_learning_prompt_path_list[few_shot_learning_question_list.index(j + 1)])\n",
    "                        print(f\"[INFO] {COT_info_header}: The few shot learning prompt is\\n{prompt}\")\n",
    "                    else:\n",
    "                        if (j + 1 == 6):\n",
    "                            prompt = build_Q6_prompt(refined_text, question)\n",
    "                        elif (j + 1 == 7):\n",
    "                            prompt = build_Q7_prompt(refined_text, question)\n",
    "                        else:\n",
    "                            prompt = build_default_prompt(refined_text, question)\n",
    "                        print(f\"[INFO] {COT_info_header}: The default prompt is\\n{prompt}\")\n",
    "\n",
    "                    #print(f\"The prompt for Q{(j + 1)} is {prompt}\")\n",
    "                    response = LLM_text(model, tokenizer, prompt)\n",
    "                    choices_list = generate_choices(question)\n",
    "                    refined_response = refine_and_check(response, choices_list, question_list, j, info_header, 0, bumpy_question_threshold)\n",
    "            else:\n",
    "                if (few_shot_learning_question_list is not None and (j + 1) in few_shot_learning_question_list):\n",
    "                    prompt = build_few_shot_learning_prompt(refined_text, question, few_shot_learning_prompt_path_list[few_shot_learning_question_list.index(j + 1)])\n",
    "                    print(f\"[INFO] {info_header}: The few shot learning prompt is\\n{prompt}\")\n",
    "                else:\n",
    "                    if (model_name != \"baseline\"):\n",
    "                        if (j + 1 == 6):\n",
    "                            prompt = build_Q6_prompt(refined_text, question)\n",
    "                        elif (j + 1 == 7):\n",
    "                            prompt = build_Q7_prompt(refined_text, question)\n",
    "                        else:\n",
    "                            prompt = build_default_prompt(refined_text, question)\n",
    "                        print(f\"[INFO] {info_header}: The default prompt is\\n{prompt}\")\n",
    "\n",
    "                #print(f\"The prompt for Q{(j + 1)} is {prompt}\")\n",
    "                if (model_name != \"baseline\"):\n",
    "                    response = LLM_text(model, tokenizer, prompt)\n",
    "                    choices_list = generate_choices(question)\n",
    "                    refined_response = refine_and_check(response, choices_list, question_list, j, info_header, 0, bumpy_question_threshold)\n",
    "                else:\n",
    "                    refined_response = baseline(refined_text, j + 1)\n",
    "            if (model_name != \"baseline\"):\n",
    "                print(f\"[INFO] {info_header}: The refined model response is {refined_response}\")\n",
    "            else:\n",
    "                print(f\"[INFO] (Baseline): The baseline extraction result is {refined_response}\")\n",
    "            if (model_name != \"baseline\" and j + 1 == 7 and refined_response == \"Not mentioned\"):\n",
    "                print(f\"[INFO]: Running the second prompt on Q7.\")\n",
    "                second_prompt = build_second_Q7_prompt(refined_text)\n",
    "                print(f\"[INFO] {info_header}: The second Q7 prompt is\\n{second_prompt}\")\n",
    "                second_response = LLM_text(model, tokenizer, second_prompt)\n",
    "                second_choices_list = [\"Yes\", \"No\"]\n",
    "                second_question_list = ['''Does the text mention the patient's occupation?\n",
    "                    Your answer must be one of the following:\n",
    "                    - Yes\n",
    "                    - No''']\n",
    "                second_refined_response = refine_and_check(second_response, second_choices_list, second_question_list, j, info_header, 0, bumpy_question_threshold)\n",
    "                if (second_refined_response == \"Yes\"):\n",
    "                    refined_response = \"Employed\"\n",
    "            responses.append(refined_response)\n",
    "            question_end_time = time.time()\n",
    "            question_time = question_end_time - question_start_time\n",
    "            print(f\"[INFO]: Processing question {counter + 1} took {question_time} seconds\")\n",
    "            time_usage.append(question_time)\n",
    "            if (question_time > bumpy_question_threshold):\n",
    "                print(f\"[INFO] {info_header}: This is a bumpy question, which takes {question_time} seconds to finish (Bumpy question threshold = {bumpy_question_threshold})\")\n",
    "            counter += 1\n",
    "        result_df[MRN] = [text] + responses\n",
    "        time_df[MRN] = time_usage\n",
    "        patient_end_time = time.time()\n",
    "        print(f\"[INFO]: Processing patient {i + 1} took {patient_end_time - patient_start_time} seconds\")\n",
    "    if (test_set):\n",
    "        result_df.to_csv(f\"SDoH extraction performance ({model_name}) (Test).csv\", index = False)\n",
    "    else:\n",
    "        result_df.to_csv(f\"SDoH extraction performance ({model_name}).csv\", index = False)\n",
    "    # The \"research_df\" is for downstream tasks (count number of cases in each SDoH)\n",
    "    research_df = result_df\n",
    "    research_df.set_index(model_name, inplace=True)\n",
    "    research_df = result_df.T\n",
    "    research_df.reset_index(inplace=True)\n",
    "    research_df.insert(2, \"Date\", update_date_list)\n",
    "    if (test_set):\n",
    "        research_df.to_csv(f\"SDoH extraction result ({model_name}) (Test).csv\", index = False)\n",
    "        time_df.to_csv(f\"SDoH extraction time ({model_name}) (Test).csv\", index = False)\n",
    "    else:\n",
    "        research_df.to_csv(f\"SDoH extraction result ({model_name}).csv\", index = False)\n",
    "        time_df.to_csv(f\"SDoH extraction time ({model_name}).csv\", index = False)\n",
    "    print(\"****************************** LOG END ******************************\\n\")\n",
    "    sys.stdout.close()\n",
    "    sys.stdout = sys.__stdout__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5483fd37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Supported model names (All non-quantized models can ONLY be run using CPUs. All quantized models MUST be run using GPUs):\n",
    "# Unquantized models (FP32)\n",
    "# openchat_3.5\n",
    "# zephyr-7b-beta\n",
    "# vicuna-7b-v1.5\n",
    "# Llama-2-7b-chat-hf\n",
    "# vicuna-13b-v1.5\n",
    "# WizardLM-13B-V1.2\n",
    "# Llama-2-13b-chat-hf\n",
    "# vicuna-33b-v1.3\n",
    "# WizardLM-70B-V1.0\n",
    "# Llama-2-70b-chat-hf\n",
    "\n",
    "# Quantized models (INT4, GEMV, AWQ, 4-bit, group size 128)\n",
    "# openchat_3.5-w4-g128\n",
    "# zephyr-7b-beta-w4-g128\n",
    "# vicuna-7b-v1.5-w4-g128\n",
    "# Llama-2-7b-chat-hf-w4-g128\n",
    "# vicuna-13b-v1.5-w4-g128\n",
    "# WizardLM-13B-V1.2-w4-g128\n",
    "# Llama-2-13b-chat-hf-w4-g128\n",
    "# vicuna-33b-v1.3-w4-g128\n",
    "# WizardLM-70B-V1.0-w3-g128\n",
    "# Llama-2-70b-chat-hf-w3-g128\n",
    "\n",
    "# Baseline model (Regular expression pattern matching)\n",
    "# baseline\n",
    "\n",
    "question_text_file_path = \"./LLM Questions.txt\"\n",
    "COT_question_text_file_path = \"./LLM Questions (COT).txt\"\n",
    "SDoH_file_path = \"./observation_fact_notes (Social Documentation) (Unique).csv\"\n",
    "first_n = None\n",
    "data = pd.read_csv(SDoH_file_path)\n",
    "test_set = True\n",
    "if test_set:\n",
    "    specific_patients = data[\"PATIENT_NUM\"].iloc[100:200].tolist()\n",
    "else:\n",
    "    specific_patients = data[\"PATIENT_NUM\"].iloc[0:100].tolist()\n",
    "few_shot_learning_question_list = None\n",
    "few_shot_learning_prompt_path_list = None\n",
    "specific_COT_questions_list = [2]\n",
    "specific_questions_list = None\n",
    "bumpy_question_threshold = 500\n",
    "model_name_list = [\"Llama-2-70b-chat-hf\"]\n",
    "#model_name_list = [\"WizardLM-13B-V1.2\"]\n",
    "for model_name in model_name_list:\n",
    "    if (model_name != \"baseline\"):\n",
    "        model, tokenizer = get_model(model_name)\n",
    "        if (\"-w4-g128\" in model_name):\n",
    "            device = torch.device(\"cuda\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "        model.to(device)\n",
    "        LLM_pipeline(model_name, question_text_file_path, COT_question_text_file_path, SDoH_file_path, first_n, specific_patients, few_shot_learning_question_list, few_shot_learning_prompt_path_list, specific_COT_questions_list, specific_questions_list, bumpy_question_threshold, test_set)\n",
    "        # Unload the model from GPU and RAM\n",
    "        del model          # Delete the model\n",
    "        gc.collect()       # Collect garbage\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()  # Clear CUDA cache\n",
    "    else:\n",
    "        LLM_pipeline(model_name, question_text_file_path, COT_question_text_file_path, SDoH_file_path, first_n, specific_patients, few_shot_learning_question_list, few_shot_learning_prompt_path_list, specific_COT_questions_list, specific_questions_list, bumpy_question_threshold, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f709ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836813c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = pd.read_csv(f\"SDoH extraction performance ({model_name}) (Test).csv\")\n",
    "check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d31d22d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daf463ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
